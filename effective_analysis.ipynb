{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiCmfUSeUQUb"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# import emoji\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk import MWETokenizer, TweetTokenizer, word_tokenize\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "# from transformers import BertTokenizer\n",
        "\n",
        "# import torch\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DA1FuTroVbOA",
        "outputId": "12526323-99a6-4890-baa1-7d4d5c1260d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñç                              | 10 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñä                             | 20 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà                            | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                          | 40 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                         | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                       | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                     | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                  | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä              | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç           | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè        | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ      | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=8aa325012bbaa23d1d9f3c7bf0e598a624cb60f4e9ff1e301a0a193db433b55f\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ahDaVToUQUg"
      },
      "outputs": [],
      "source": [
        "with open('data/dumped/tweets.pkl','rb') as f:\n",
        "    X, y = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgic3MG4UQUg",
        "outputId": "053661c5-0df2-44c7-eb2d-4af54a640eac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "420"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LViIZ8MRUQUg"
      },
      "source": [
        "## Average Tweet Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O1V3CDrUQUh"
      },
      "outputs": [],
      "source": [
        "get_tweets_length = lambda doc: np.array(\n",
        "    [len(tweet) for tweet in doc], dtype=np.int32)\n",
        "\n",
        "def get_categorical_tweets_length(label, docs=X, labels=y):\n",
        "    \"\"\"\n",
        "    Returns a list of tweets lengths for each category.\n",
        "    \"\"\"\n",
        "    tweets_length = np.empty(docs[labels == label].shape, dtype=np.int32)\n",
        "    for index, user in enumerate(docs[labels == label]):\n",
        "        tweets_length[index, :] = get_tweets_length(user)\n",
        "\n",
        "    return tweets_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "JuAWIzO7UQUi",
        "outputId": "2109df79-77d3-4f94-cf65-3d8299dfeebf"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "i_tweets_length_per_user = np.mean(get_categorical_tweets_length('I'), axis=1)\n",
        "ni_tweets_length_per_user = np.mean(get_categorical_tweets_length('NI'), axis=1)\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Irony')\n",
        "plt.xlabel('Avg. tweet length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0, 55])\n",
        "plt.hist(i_tweets_length_per_user, 21, range=[75, 285]);\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.tight_layout()\n",
        "plt.title('Non-irony')\n",
        "plt.xlabel('Avg. tweet length')\n",
        "plt.ylabel('Frequency')\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0, 55])\n",
        "plt.hist(ni_tweets_length_per_user, 21, range=[75, 285]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSHBGt7GUQUj",
        "outputId": "6ce75c64-79de-4fbf-e08f-2aadef7779cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(136.01752380952382, 156.02366666666668)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(i_tweets_length_per_user), np.mean(ni_tweets_length_per_user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC_DMa9LUQUj",
        "outputId": "b610c3f8-cf6c-4957-c85b-1a73f62157b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(134.5125, 160.1525)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.median(i_tweets_length_per_user), np.median(ni_tweets_length_per_user)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp2DLeHCUQUk"
      },
      "source": [
        "## Emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTEhWQFdUQUk"
      },
      "outputs": [],
      "source": [
        "extract_emojis = lambda text: [d['emoji'] for d in emoji.emoji_list(text)]\n",
        "emoji_dict = dict([(i, 0) for i in list(emoji.EMOJI_DATA.keys())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbwevkkuUQUk"
      },
      "outputs": [],
      "source": [
        "def get_emojis(label, emoji_dict=emoji_dict, doc=X, labels=y):\n",
        "    \"\"\"\n",
        "    Returns: a) a list of vector for emojis from the given label\n",
        "             b) a list of number of emojis from users\n",
        "    \"\"\"\n",
        "    emojis, counts = [], []\n",
        "    for user in doc[labels == label]:\n",
        "        user_counts = 0\n",
        "        user_emojis = emoji_dict.copy()\n",
        "        for tweet in user:\n",
        "            extracted_emoji = extract_emojis(tweet)\n",
        "            user_counts += len(extracted_emoji)\n",
        "            for emoji in extracted_emoji:\n",
        "                user_emojis[emoji] += 1\n",
        "        emojis.append(user_emojis)\n",
        "        counts.append(user_counts)\n",
        "\n",
        "    return emojis, counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cs8ijDhUQUl"
      },
      "outputs": [],
      "source": [
        "i_emojis, i_emoji_counts = get_emojis('I')\n",
        "ni_emojis, ni_emoji_counts = get_emojis('NI')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "3QrmVPFyUQUl",
        "outputId": "b4ec12c0-1feb-44ad-f7a9-df2fa7a5d650"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplot(2, 1, 1)\n",
        "plt.title('Irony')\n",
        "plt.xlabel('Total Emoji count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0, 90])\n",
        "plt.hist(i_emoji_counts, 30, range=[0, 300]);\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title('Non-irony')\n",
        "plt.xlabel('Total Emoji count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0, 90])\n",
        "plt.hist(ni_emoji_counts, 30, range=[0, 300]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyGntgVQUQUl"
      },
      "outputs": [],
      "source": [
        "emoji_vectors = list(emoji_dict.keys())\n",
        "\n",
        "get_categorical_emojis_vector = lambda d: np.sum(\n",
        "    [np.array(list(emojis.values())) for emojis in d], axis=0)\n",
        "\n",
        "i_emoji_vectors = get_categorical_emojis_vector(i_emojis)\n",
        "ni_emoji_vectors = get_categorical_emojis_vector(ni_emojis)\n",
        "\n",
        "get_sorted_emoji_dict = lambda categorical_vec, emoji_vec=emoji_vectors: dict(\n",
        "    sorted([(emoji_vec[i], categorical_vec[i]) for i in range(\n",
        "        len(emoji_vec))], key=lambda x: x[1], reverse=True))\n",
        "\n",
        "i_emojis_dict = get_sorted_emoji_dict(i_emoji_vectors)\n",
        "ni_emojis_dict = get_sorted_emoji_dict(ni_emoji_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcxCTzo-UQUl",
        "outputId": "c0d12cef-89a4-4fde-bd89-ad613505d6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['üòÇ', 'ü§£', 'ü§î', 'üôÑ', 'üòâ', 'üëç', 'üëá', '‚ù§Ô∏è', 'ü§¶\\u200d‚ôÇÔ∏è', 'ü§∑\\u200d‚ôÇÔ∏è', 'üòí', 'üòÑ', 'üòè', 'üòÅ', 'üòä']\n"
          ]
        }
      ],
      "source": [
        "print(list(i_emojis_dict.keys())[:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9WFCy50UQUm",
        "outputId": "bf83553b-0c44-444b-bcc4-ca52c1b9a2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['üòÇ', 'üò≠', 'ü§£', '‚úÖ', 'üî•', '‚ù§Ô∏è', 'ü§î', 'üôè', 'üôÑ', 'üá∫üá∏', 'üíÄ', 'üöÄ', 'üëá', 'üò©', 'üá∫üá¶']\n"
          ]
        }
      ],
      "source": [
        "print(list(ni_emojis_dict.keys())[:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDqo4cNZUQUm"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FrayfzPUQUm"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(n_estimators=2000)\n",
        "svm = SVC(kernel='rbf', C=1.0, random_state=42)\n",
        "logistic_regression = LogisticRegression(C=1.0, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9DW0wG8UQUm"
      },
      "source": [
        "### Average Tweet Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tH7PfCcUQUm",
        "outputId": "a0d86d09-bae1-4816-be4c-e6e4647436b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.6309523809523809 \t SVM: 0.7738095238095238 \t LR: 0.7738095238095238\n",
            "RF: 0.5952380952380952 \t SVM: 0.7261904761904762 \t LR: 0.7023809523809523\n",
            "RF: 0.4523809523809524 \t SVM: 0.6547619047619048 \t LR: 0.6190476190476191\n",
            "RF: 0.6309523809523809 \t SVM: 0.6666666666666666 \t LR: 0.6428571428571429\n",
            "RF: 0.6428571428571429 \t SVM: 0.6904761904761905 \t LR: 0.6666666666666666\n",
            "\n",
            "AVG RF SCORE: 0.5904761904761904 \n",
            "AVG SVM SCORE: 0.7023809523809523 \n",
            "AVG LR SCORE: 0.680952380952381\n"
          ]
        }
      ],
      "source": [
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in train_labels_cv])\n",
        "    test_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in test_labels_cv])\n",
        "\n",
        "    train_average_length = np.mean(\n",
        "        np.array([get_tweets_length(user) for user in train_data_cv]),\n",
        "        axis=1).reshape(-1, 1)\n",
        "\n",
        "    forest.fit(train_average_length, train_labels_cv)\n",
        "    svm.fit(train_average_length, train_labels_cv)\n",
        "    logistic_regression.fit(train_average_length, train_labels_cv)\n",
        "\n",
        "    test_average_length = np.mean(\n",
        "        np.array([get_tweets_length(user) for user in test_data_cv]),\n",
        "        axis=1).reshape(-1, 1)\n",
        "\n",
        "    forest_score = forest.score(test_average_length, test_labels_cv)\n",
        "    svm_score = svm.score(test_average_length, test_labels_cv)\n",
        "    logistic_regression_score = logistic_regression.score(\n",
        "        test_average_length, test_labels_cv)\n",
        "    \n",
        "    print('RF:', forest_score, '\\t', 'SVM:', svm_score, '\\t', \n",
        "          'LR:', logistic_regression_score)\n",
        "\n",
        "    # index = np.where(prediction != test_labels_cv)[0]\n",
        "    # wrong_index.append(index)\n",
        "    # print(classification_report(test_labels_cv, prediction))\n",
        "\n",
        "    scores.append((forest_score, svm_score, logistic_regression_score))\n",
        "\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "print('\\nAVG RF SCORE:', mean_scores[0],\n",
        "      '\\nAVG SVM SCORE:', mean_scores[1],\n",
        "      '\\nAVG LR SCORE:', mean_scores[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVi-KcjoUQUn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9gAqjHwUQUn"
      },
      "source": [
        "### Emoji Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY6zo0fRUQUn"
      },
      "outputs": [],
      "source": [
        "def get_emoji_usage(user, emoji_dict=emoji_dict):\n",
        "    \"\"\"\n",
        "    Returns: a) a list of vector for emojis from the given profile\n",
        "             b) a list of number of emojis from this user\n",
        "    \"\"\"\n",
        "    counts = 0\n",
        "    user_emoji_dict = copy.deepcopy(emoji_dict)\n",
        "\n",
        "    for tweet in user:\n",
        "        extracted_emoji = extract_emojis(tweet)\n",
        "        counts += len(extracted_emoji)\n",
        "        for emoji in extracted_emoji:\n",
        "            user_emoji_dict[emoji] += 1\n",
        "\n",
        "    emojis = np.array(list(user_emoji_dict.values()))\n",
        "\n",
        "    return emojis, counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww35he2CUQUn"
      },
      "source": [
        "#### Emoji Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es1o3WasUQUn",
        "outputId": "2a4a7543-c0e5-4ab9-a5ed-d0f1cc6cc2d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.6428571428571429 \t SVM: 0.6190476190476191 \t LR: 0.4523809523809524\n",
            "RF: 0.5476190476190477 \t SVM: 0.5476190476190477 \t LR: 0.5476190476190477\n",
            "RF: 0.6547619047619048 \t SVM: 0.6785714285714286 \t LR: 0.5833333333333334\n",
            "RF: 0.7023809523809523 \t SVM: 0.6071428571428571 \t LR: 0.5595238095238095\n",
            "RF: 0.5119047619047619 \t SVM: 0.5714285714285714 \t LR: 0.5238095238095238\n",
            "\n",
            "AVG RF SCORE: 0.6119047619047618 \n",
            "AVG SVM SCORE: 0.6047619047619047 \n",
            "AVG LR SCORE: 0.5333333333333334\n"
          ]
        }
      ],
      "source": [
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv_original, test_labels_cv_original = y[train], y[test]\n",
        "\n",
        "    train_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in train_labels_cv_original])\n",
        "    test_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in test_labels_cv_original])\n",
        "\n",
        "    train_emoji_usage = np.array(\n",
        "        [get_emoji_usage(user)[1] for user in train_data_cv]).reshape(-1, 1)\n",
        "\n",
        "    forest.fit(train_emoji_usage, train_labels_cv)\n",
        "    svm.fit(train_emoji_usage, train_labels_cv)\n",
        "    logistic_regression.fit(train_emoji_usage, train_labels_cv)\n",
        "\n",
        "    test_emoji_usage = np.array(\n",
        "        [get_emoji_usage(user)[1] for user in test_data_cv]).reshape(-1, 1)\n",
        "\n",
        "    forest_score = forest.score(test_emoji_usage, test_labels_cv)\n",
        "    svm_score = svm.score(test_emoji_usage, test_labels_cv)\n",
        "    logistic_regression_score = logistic_regression.score(\n",
        "        test_emoji_usage, test_labels_cv)\n",
        "    \n",
        "    print('RF:', forest_score, '\\t', 'SVM:', svm_score, '\\t', \n",
        "          'LR:', logistic_regression_score)\n",
        "\n",
        "    # index = np.where(prediction != test_labels_cv)[0]\n",
        "    # wrong_index.append(index)\n",
        "    # print(classification_report(test_labels_cv, prediction))\n",
        "\n",
        "    scores.append((forest_score, svm_score, logistic_regression_score))\n",
        "\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "print('\\nAVG RF SCORE:', mean_scores[0],\n",
        "      '\\nAVG SVM SCORE:', mean_scores[1],\n",
        "      '\\nAVG LR SCORE:', mean_scores[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA-jN9G2UQUn"
      },
      "source": [
        "#### Emoji Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZasVLiPUQUn",
        "outputId": "a0bc1fd1-8100-49d2-9c37-ca6cf8737fc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.7261904761904762 \t SVM: 0.6904761904761905 \t LR: 0.5952380952380952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.7738095238095238 \t SVM: 0.6785714285714286 \t LR: 0.6904761904761905\n",
            "RF: 0.7738095238095238 \t SVM: 0.7142857142857143 \t LR: 0.6904761904761905\n",
            "RF: 0.6904761904761905 \t SVM: 0.6547619047619048 \t LR: 0.6309523809523809\n",
            "RF: 0.6190476190476191 \t SVM: 0.6904761904761905 \t LR: 0.6071428571428571\n",
            "\n",
            "AVG RF SCORE: 0.7166666666666667 \n",
            "AVG SVM SCORE: 0.6857142857142857 \n",
            "AVG LR SCORE: 0.6428571428571428\n"
          ]
        }
      ],
      "source": [
        "# No Optimization; Time Killer!\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv_original, test_labels_cv_original = y[train], y[test]\n",
        "\n",
        "    train_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in train_labels_cv_original])\n",
        "    test_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in test_labels_cv_original])\n",
        "\n",
        "    train_emoji_usage = np.array(\n",
        "        [get_emoji_usage(user)[0] for user in train_data_cv])\n",
        "\n",
        "    forest.fit(train_emoji_usage, train_labels_cv)\n",
        "    svm.fit(train_emoji_usage, train_labels_cv)\n",
        "    logistic_regression.fit(train_emoji_usage, train_labels_cv)\n",
        "\n",
        "    test_emoji_usage = np.array(\n",
        "        [get_emoji_usage(user)[0] for user in test_data_cv])\n",
        "\n",
        "    forest_score = forest.score(test_emoji_usage, test_labels_cv)\n",
        "    svm_score = svm.score(test_emoji_usage, test_labels_cv)\n",
        "    logistic_regression_score = logistic_regression.score(\n",
        "        test_emoji_usage, test_labels_cv)\n",
        "    \n",
        "    print('RF:', forest_score, '\\t', 'SVM:', svm_score, '\\t', \n",
        "          'LR:', logistic_regression_score)\n",
        "\n",
        "    # index = np.where(prediction != test_labels_cv)[0]\n",
        "    # wrong_index.append(index)\n",
        "    # print(classification_report(test_labels_cv, prediction))\n",
        "\n",
        "    scores.append((forest_score, svm_score, logistic_regression_score))\n",
        "\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "print('\\nAVG RF SCORE:', mean_scores[0],\n",
        "      '\\nAVG SVM SCORE:', mean_scores[1],\n",
        "      '\\nAVG LR SCORE:', mean_scores[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xT01MHWUQUo"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSID3v6RUQUo"
      },
      "source": [
        "#### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZm9Pl7hUQUo",
        "outputId": "cc983bb1-e1c1-419c-d8b5-9611f265336b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "special_tokens = ['#USER#', '#HASHTAG#', '#URL#']\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': special_tokens}\n",
        "bert_tokenizer.add_special_tokens(special_tokens_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMEFNh6NUQUo",
        "outputId": "59bd8d90-b5fb-4b62-a086-0fd4a59e8bc6"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCanceled future for execute_request message before replies were done"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# No Optimization; Time Killer!\n",
        "### words\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv_original, test_labels_cv_original = y[train], y[test]\n",
        "\n",
        "    bert_tokens_dict = {}\n",
        "    for user in train_data_cv:\n",
        "        for tweet in user:\n",
        "            tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "            for token in tokens:\n",
        "                bert_tokens_dict[token] = 0\n",
        "\n",
        "\n",
        "    train_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in train_labels_cv_original])\n",
        "    test_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in test_labels_cv_original])\n",
        "\n",
        "    ##### train\n",
        "    trains_vector_cv = np.zeros((\n",
        "        len(train_data_cv), len(bert_tokens_dict)))\n",
        "    for index, user in enumerate(train_data_cv):\n",
        "        user_tokens = copy.deepcopy(bert_tokens_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "            for token in tokens:\n",
        "                user_tokens[token] += 1\n",
        "        trains_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    forest.fit(trains_vector_cv, train_labels_cv)\n",
        "    svm.fit(trains_vector_cv, train_labels_cv)\n",
        "    logistic_regression.fit(trains_vector_cv, train_labels_cv)\n",
        "\n",
        "    ##### validation\n",
        "    tests_vector_cv = np.zeros((\n",
        "        len(test_data_cv), len(bert_tokens_dict)))\n",
        "    for index, user in enumerate(test_data_cv):\n",
        "        user_tokens = copy.deepcopy(bert_tokens_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "            for token in tokens:\n",
        "                try:\n",
        "                    user_tokens[token] += 1\n",
        "                except KeyError:\n",
        "                    pass\n",
        "        tests_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    forest_score = forest.score(tests_vector_cv, test_labels_cv)\n",
        "    svm_score = svm.score(tests_vector_cv, test_labels_cv)\n",
        "    logistic_regression_score = logistic_regression.score(\n",
        "        tests_vector_cv, test_labels_cv)\n",
        "    \n",
        "    print('RF:', forest_score, '\\t', 'SVM:', svm_score, '\\t', \n",
        "          'LR:', logistic_regression_score)\n",
        "\n",
        "    # index = np.where(prediction != test_labels_cv)[0]\n",
        "    # wrong_index.append(index)\n",
        "    # print(classification_report(test_labels_cv, prediction))\n",
        "\n",
        "    scores.append((forest_score, svm_score, logistic_regression_score))\n",
        "\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "print('\\nAVG RF SCORE:', mean_scores[0],\n",
        "      '\\nAVG SVM SCORE:', mean_scores[1],\n",
        "      '\\nAVG LR SCORE:', mean_scores[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbmVRHu4UQUo"
      },
      "outputs": [],
      "source": [
        "bert_tokens_dict = {}\n",
        "for user in train_data_cv:\n",
        "    for tweet in user:\n",
        "        tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "        for token in tokens:\n",
        "            bert_tokens_dict[token] = 0\n",
        "\n",
        "trains_vector_cv = np.zeros((\n",
        "        len(train_data_cv), len(bert_tokens_dict)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auDjBXuFUQUo"
      },
      "outputs": [],
      "source": [
        "for index, user in enumerate(train_data_cv):\n",
        "    user_tokens = copy.deepcopy(bert_tokens_dict)\n",
        "    for tweet in user:\n",
        "        tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "        for token in tokens:\n",
        "            user_tokens[token] += 1\n",
        "    trains_vector_cv[index] = np.array(\n",
        "                    list(user_tokens.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH0cA1l2UQUp",
        "outputId": "1948643b-9d5b-4436-8a01-4f3c57754a6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(336, 24515)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trains_vector_cv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkWElRWNUQUp"
      },
      "outputs": [],
      "source": [
        "trains_vector_cv = np.zeros((\n",
        "        len(train_data_cv), len(bert_tokens_dict)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Ma2klVUQUp",
        "outputId": "d0a9d35f-8b42-4a90-b313-42b0acc80d5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(336, 24515)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trains_vector_cv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPubKV79UQUp",
        "outputId": "87df7fd5-cd2d-4e56-9484-9d132c02e991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['#USER#', '#USER#', '#USER#', 'putin', 'is', 'mentally', 'un', '##hing', '##ed', ',', 'he', 'needs', 'to', 'be', 'in', 'a', 'strait', 'jacket', '.']\n"
          ]
        }
      ],
      "source": [
        "print([token for token in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUV4rVUOUQUp",
        "outputId": "a396d948-1f04-4439-ecc2-4c15a365aaad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24515"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(bert_tokens_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGOQz879UQUp"
      },
      "source": [
        "#### MWE Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5bJ8TJ0UQUp"
      },
      "outputs": [],
      "source": [
        "mwe_tokenizer = MWETokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34Wqxs-kUQUp",
        "outputId": "0fd046d7-56d4-4718-d5b1-e5163f03bb4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9404761904761905 \t SVM: 0.8928571428571429 \t LR: 0.9166666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8809523809523809 \t SVM: 0.9047619047619048 \t LR: 0.9404761904761905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8690476190476191 \t SVM: 0.8928571428571429 \t LR: 0.9166666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8690476190476191 \t SVM: 0.8571428571428571 \t LR: 0.8571428571428571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9642857142857143 \t SVM: 0.8809523809523809 \t LR: 0.9047619047619048\n",
            "\n",
            "AVG RF SCORE: 0.9047619047619048 \n",
            "AVG SVM SCORE: 0.8857142857142858 \n",
            "AVG LR SCORE: 0.9071428571428571\n"
          ]
        }
      ],
      "source": [
        "# No Optimization; Time Killer!\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv_original, test_labels_cv_original = y[train], y[test]\n",
        "\n",
        "    mwe_tokens_dict = {}\n",
        "    for user in train_data_cv:\n",
        "        for tweet in user:\n",
        "            tokens = set(mwe_tokenizer.tokenize(word_tokenize(tweet.lower())))\n",
        "            for token in tokens:\n",
        "                mwe_tokens_dict[token] = 0\n",
        "\n",
        "    train_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in train_labels_cv_original])\n",
        "    test_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in test_labels_cv_original])\n",
        "\n",
        "    trains_vector_cv = np.zeros((\n",
        "        len(train_data_cv), len(mwe_tokens_dict)))\n",
        "    for index, user in enumerate(train_data_cv):\n",
        "        user_tokens = copy.deepcopy(mwe_tokens_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(mwe_tokenizer.tokenize(word_tokenize(tweet.lower())))\n",
        "            for token in tokens:\n",
        "                user_tokens[token] += 1\n",
        "        trains_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    forest.fit(trains_vector_cv, train_labels_cv)\n",
        "    svm.fit(trains_vector_cv, train_labels_cv)\n",
        "    logistic_regression.fit(trains_vector_cv, train_labels_cv)\n",
        "\n",
        "    tests_vector_cv = np.zeros((\n",
        "        len(test_data_cv), len(mwe_tokens_dict)))\n",
        "    for index, user in enumerate(test_data_cv):\n",
        "        user_tokens = copy.deepcopy(mwe_tokens_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(mwe_tokenizer.tokenize(word_tokenize(tweet.lower())))\n",
        "            for token in tokens:\n",
        "                try:\n",
        "                    user_tokens[token] += 1\n",
        "                except KeyError:\n",
        "                    pass\n",
        "        tests_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    forest_score = forest.score(tests_vector_cv, test_labels_cv)\n",
        "    svm_score = svm.score(tests_vector_cv, test_labels_cv)\n",
        "    logistic_regression_score = logistic_regression.score(\n",
        "        tests_vector_cv, test_labels_cv)\n",
        "    \n",
        "    print('RF:', forest_score, '\\t', 'SVM:', svm_score, '\\t', \n",
        "          'LR:', logistic_regression_score)\n",
        "\n",
        "    # index = np.where(prediction != test_labels_cv)[0]\n",
        "    # wrong_index.append(index)\n",
        "    # print(classification_report(test_labels_cv, prediction))\n",
        "\n",
        "    scores.append((forest_score, svm_score, logistic_regression_score))\n",
        "\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "print('\\nAVG RF SCORE:', mean_scores[0],\n",
        "      '\\nAVG SVM SCORE:', mean_scores[1],\n",
        "      '\\nAVG LR SCORE:', mean_scores[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6acIo5iTUQUq",
        "outputId": "aead3b52-f455-4672-a21a-b0ed314a7992"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "73994"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(mwe_tokens_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdDZkHkoUQUq"
      },
      "source": [
        "#### Twitter Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKQNMc_4UQUq"
      },
      "outputs": [],
      "source": [
        "twitter_tokenizer = TweetTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8I_ANUIUQUq",
        "outputId": "64fc3cd4-f591-40d3-81b3-fa2bcad1d825"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9523809523809523 \t SVM: 0.9047619047619048 \t LR: 0.9166666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8928571428571429 \t SVM: 0.9047619047619048 \t LR: 0.9404761904761905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8690476190476191 \t SVM: 0.8809523809523809 \t LR: 0.9166666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8571428571428571 \t SVM: 0.8452380952380952 \t LR: 0.8571428571428571\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9761904761904762 \t SVM: 0.8809523809523809 \t LR: 0.9047619047619048\n",
            "\n",
            "AVG RF SCORE: 0.9095238095238095 \n",
            "AVG SVM SCORE: 0.8833333333333334 \n",
            "AVG LR SCORE: 0.9071428571428571\n"
          ]
        }
      ],
      "source": [
        "# No Optimization; Time Killer!\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv_original, test_labels_cv_original = y[train], y[test]\n",
        "\n",
        "    twitter_token_dict = {}\n",
        "    for user in train_data_cv:\n",
        "        for tweet in user:\n",
        "            tokens = set(twitter_tokenizer.tokenize(tweet.lower()))\n",
        "            for token in tokens:\n",
        "                twitter_token_dict[token] = 0\n",
        "\n",
        "    train_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in train_labels_cv_original])\n",
        "    test_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in test_labels_cv_original])\n",
        "\n",
        "    trains_vector_cv = np.zeros((\n",
        "        len(train_data_cv), len(twitter_token_dict)))\n",
        "    for index, user in enumerate(train_data_cv):\n",
        "        user_tokens = copy.deepcopy(twitter_token_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(twitter_tokenizer.tokenize(tweet.lower()))\n",
        "            for token in tokens:\n",
        "                user_tokens[token] += 1\n",
        "        trains_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    forest.fit(trains_vector_cv, train_labels_cv)\n",
        "    svm.fit(trains_vector_cv, train_labels_cv)\n",
        "    logistic_regression.fit(trains_vector_cv, train_labels_cv)\n",
        "\n",
        "    tests_vector_cv = np.zeros((\n",
        "        len(test_data_cv), len(twitter_token_dict)))\n",
        "    for index, user in enumerate(test_data_cv):\n",
        "        user_tokens = copy.deepcopy(twitter_token_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(twitter_tokenizer.tokenize(tweet.lower()))\n",
        "            for token in tokens:\n",
        "                try:\n",
        "                    user_tokens[token] += 1\n",
        "                except KeyError:\n",
        "                    pass\n",
        "        tests_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    forest_score = forest.score(tests_vector_cv, test_labels_cv)\n",
        "    svm_score = svm.score(tests_vector_cv, test_labels_cv)\n",
        "    logistic_regression_score = logistic_regression.score(\n",
        "        tests_vector_cv, test_labels_cv)\n",
        "    \n",
        "    print('RF:', forest_score, '\\t', 'SVM:', svm_score, '\\t', \n",
        "          'LR:', logistic_regression_score)\n",
        "\n",
        "    # index = np.where(prediction != test_labels_cv)[0]\n",
        "    # wrong_index.append(index)\n",
        "    # print(classification_report(test_labels_cv, prediction))\n",
        "\n",
        "    scores.append((forest_score, svm_score, logistic_regression_score))\n",
        "\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "print('\\nAVG RF SCORE:', mean_scores[0],\n",
        "      '\\nAVG SVM SCORE:', mean_scores[1],\n",
        "      '\\nAVG LR SCORE:', mean_scores[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjkUr63BUQUq",
        "outputId": "5d4a1521-f689-46b4-ba7b-c6b8ad8ab81d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "67889"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(twitter_token_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mpnNZPSUQUq"
      },
      "source": [
        "### Concatenation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzvkIx3hUQUq",
        "outputId": "d640ed9d-27e9-4436-c5b3-658e683e0b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9404761904761905 \t SVM: 0.9047619047619048 \t LR: 0.9166666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8928571428571429 \t SVM: 0.9047619047619048 \t LR: 0.9404761904761905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8809523809523809 \t SVM: 0.8809523809523809 \t LR: 0.9047619047619048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8809523809523809 \t SVM: 0.8571428571428571 \t LR: 0.9047619047619048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9761904761904762 \t SVM: 0.8809523809523809 \t LR: 0.9285714285714286\n",
            "\n",
            "AVG RF SCORE: 0.9142857142857143 \n",
            "AVG SVM SCORE: 0.8857142857142858 \n",
            "AVG LR SCORE: 0.919047619047619\n"
          ]
        }
      ],
      "source": [
        "# No Optimization; Time Killer!\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv_original, test_labels_cv_original = y[train], y[test]\n",
        "\n",
        "    bert_tokens_dict = {}\n",
        "    for user in train_data_cv:\n",
        "        for tweet in user:\n",
        "            tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "            for token in tokens:\n",
        "                bert_tokens_dict[token] = 0\n",
        "\n",
        "    train_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in train_labels_cv_original])\n",
        "    test_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in test_labels_cv_original])\n",
        "\n",
        "    trains_vector_cv = np.zeros((\n",
        "        len(train_data_cv), len(bert_tokens_dict)))\n",
        "    for index, user in enumerate(train_data_cv):\n",
        "        user_tokens = copy.deepcopy(bert_tokens_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "            for token in tokens:\n",
        "                user_tokens[token] += 1\n",
        "        trains_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    train_emoji_usage = np.array(\n",
        "        [get_emoji_usage(user)[0] for user in train_data_cv])\n",
        "    \n",
        "    train_average_length = np.mean(\n",
        "        np.array([get_tweets_length(user) for user in train_data_cv]),\n",
        "        axis=1).reshape(-1, 1)\n",
        "    \n",
        "    train_cv = np.hstack((trains_vector_cv,\n",
        "        train_emoji_usage, train_average_length))\n",
        "\n",
        "    forest.fit(train_cv, train_labels_cv)\n",
        "    svm.fit(train_cv, train_labels_cv)\n",
        "    logistic_regression.fit(train_cv, train_labels_cv)\n",
        "\n",
        "    tests_vector_cv = np.zeros((\n",
        "        len(test_data_cv), len(bert_tokens_dict)))\n",
        "    for index, user in enumerate(test_data_cv):\n",
        "        user_tokens = copy.deepcopy(bert_tokens_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "            for token in tokens:\n",
        "                try:\n",
        "                    user_tokens[token] += 1\n",
        "                except KeyError:\n",
        "                    pass\n",
        "        tests_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    test_emoji_usage = np.array(\n",
        "        [get_emoji_usage(user)[0] for user in test_data_cv])\n",
        "\n",
        "    test_average_length = np.mean(\n",
        "        np.array([get_tweets_length(user) for user in test_data_cv]),\n",
        "        axis=1).reshape(-1, 1)\n",
        "\n",
        "    test_cv = np.hstack((tests_vector_cv,\n",
        "        test_emoji_usage, test_average_length))\n",
        "    \n",
        "    forest_score = forest.score(test_cv, test_labels_cv)\n",
        "    svm_score = svm.score(test_cv, test_labels_cv)\n",
        "    logistic_regression_score = logistic_regression.score(\n",
        "        test_cv, test_labels_cv)\n",
        "    \n",
        "    print('RF:', forest_score, '\\t', 'SVM:', svm_score, '\\t', \n",
        "          'LR:', logistic_regression_score)\n",
        "\n",
        "    # index = np.where(prediction != test_labels_cv)[0]\n",
        "    # wrong_index.append(index)\n",
        "    # print(classification_report(test_labels_cv, prediction))\n",
        "\n",
        "    scores.append((forest_score, svm_score, logistic_regression_score))\n",
        "\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "print('\\nAVG RF SCORE:', mean_scores[0],\n",
        "      '\\nAVG SVM SCORE:', mean_scores[1],\n",
        "      '\\nAVG LR SCORE:', mean_scores[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWU7nkleUQUr"
      },
      "source": [
        "### Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp4yePeWUQUr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuSLkimoUQUr"
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKhwV8hYUQUr"
      },
      "outputs": [],
      "source": [
        "def load_embedding(filename):\n",
        "    '''\n",
        "    Load Word Embedding: return dict, voc size, and dim of embedding\n",
        "    '''\n",
        "    embedding_dict = {}\n",
        "    with open(filename,'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            if '\\x85' in line:\n",
        "                split_line = ['\\x85'] + line.split()\n",
        "            else:\n",
        "                split_line = line.split()\n",
        "            word = split_line[0]\n",
        "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "            embedding_dict[word] = embedding\n",
        "    return embedding_dict, len(embedding_dict), len(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjfUG387UQUr"
      },
      "outputs": [],
      "source": [
        "def get_embedding(profiles,\n",
        "    tokenizer,\n",
        "    embedding_dict,\n",
        "    embedding_dim):\n",
        "    '''\n",
        "    Get embedding for profiles\n",
        "    Input:  profiles: list of profiles\n",
        "            tokenizer: the method to obtain tokens\n",
        "            embedding_dict: dict of embedding\n",
        "            embedding_dim: dimension of embedding\n",
        "    Return: embedding: numpy array of embedding\n",
        "    Required: numpy\n",
        "    '''\n",
        "    embedding = np.zeros((profiles.shape[0], embedding_dim))\n",
        "    averaging_embedding = np.zeros((profiles.shape[0], embedding_dim))\n",
        "    norm_embedding = np.zeros((profiles.shape[0], embedding_dim))\n",
        "\n",
        "    for index, user in enumerate(profiles):\n",
        "        token_counts = 0\n",
        "        for tweet in user:\n",
        "            for token in set(tokenizer(str.lower(tweet))):\n",
        "                try:\n",
        "                    embedding[index] += embedding_dict[token]\n",
        "                    token_counts += 1\n",
        "                except KeyError:\n",
        "                    pass\n",
        "        if np.linalg.norm(embedding[index]) != 0:\n",
        "            norm_embedding[index] = \\\n",
        "                embedding[index] / np.linalg.norm(embedding[index])\n",
        "            averaging_embedding[index] = \\\n",
        "                embedding[index] / token_counts\n",
        "\n",
        "    return embedding, averaging_embedding, norm_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO0-P0fFUQUr"
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(n_estimators=2000)\n",
        "avg_forest = RandomForestClassifier(n_estimators=2000)\n",
        "norm_forest = RandomForestClassifier(n_estimators=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE9GBrpHUQUr"
      },
      "source": [
        "#### Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c0iGmTf1UQUr",
        "outputId": "2bb8c118-ff4b-4174-abd1-72140f631fc8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Language processing 2/code'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cjN1t_sUQUr"
      },
      "outputs": [],
      "source": [
        "# Note that we can also try 300d embedding\n",
        "glove_path = 'data/glove/glove.twitter.27B.50d.txt'\n",
        "glove_dict, glove_counts, glove_dim = load_embedding(glove_path)\n",
        "get_glove_embedding = lambda profiles, tokenizer: \\\n",
        "    get_embedding(profiles=profiles, tokenizer=tokenizer, \n",
        "        embedding_dict=glove_dict, embedding_dim=glove_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRgp_IG4UQUr",
        "outputId": "4a40871b-8728-4211-e8b9-0e1dff6ed22d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "glove_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgGBUV1YcGXy",
        "outputId": "daa50678-0bb0-4187-935e-a1cb09ea5abf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(glove_dict[token])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duLly8b2UQUr",
        "outputId": "1f754fde-18a7-4d13-bdc9-c8f1fc7823c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1193515"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "glove_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRgPCzuGUQUr"
      },
      "outputs": [],
      "source": [
        "# Twitter\n",
        "\n",
        "matched_tokens = {}\n",
        "missed_tokens = {}\n",
        "\n",
        "for profile in X:\n",
        "    for tweet in profile:\n",
        "        tokens = set(\n",
        "            twitter_tokenizer.tokenize(tweet.lower()))\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                matched_tokens[token] = glove_dict[token]\n",
        "            except KeyError:\n",
        "                missed_tokens[token] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bh8S3PWUQUs",
        "outputId": "778a7605-a8b4-466d-9b87-401615576448"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50336, 26939)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens), len(missed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5TYBXebUQUs",
        "outputId": "918a5ab9-bc94-4ff5-db39-33767e060de7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6513879003558719"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens) / (len(matched_tokens) + len(missed_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoZa4BMIcjkS"
      },
      "outputs": [],
      "source": [
        "df_uniq = pd.read_csv(\"/content/drive/MyDrive/CS3/Zuco_uniq_o.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "lN7cIJ_ygPXu",
        "outputId": "98caed4a-83b1-464c-90bd-05f2eb411f84"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ad470571-8206-4fc1-b450-d8fbc8bc818d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>nFix</th>\n",
              "      <th>FFD</th>\n",
              "      <th>GPT</th>\n",
              "      <th>TRT</th>\n",
              "      <th>fixProp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>8.089145</td>\n",
              "      <td>2.037560</td>\n",
              "      <td>3.279184</td>\n",
              "      <td>2.620190</td>\n",
              "      <td>46.250203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>of</td>\n",
              "      <td>5.558040</td>\n",
              "      <td>1.492221</td>\n",
              "      <td>2.538022</td>\n",
              "      <td>1.791782</td>\n",
              "      <td>33.937491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and</td>\n",
              "      <td>7.638862</td>\n",
              "      <td>1.941896</td>\n",
              "      <td>3.306894</td>\n",
              "      <td>2.450817</td>\n",
              "      <td>44.722639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a</td>\n",
              "      <td>5.309940</td>\n",
              "      <td>1.491783</td>\n",
              "      <td>2.447444</td>\n",
              "      <td>1.775713</td>\n",
              "      <td>32.829983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in</td>\n",
              "      <td>5.313665</td>\n",
              "      <td>1.489553</td>\n",
              "      <td>2.466216</td>\n",
              "      <td>1.798441</td>\n",
              "      <td>32.379519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4395</th>\n",
              "      <td>swedish</td>\n",
              "      <td>24.137931</td>\n",
              "      <td>4.171965</td>\n",
              "      <td>6.316368</td>\n",
              "      <td>7.849067</td>\n",
              "      <td>83.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4396</th>\n",
              "      <td>curves</td>\n",
              "      <td>14.942529</td>\n",
              "      <td>3.625554</td>\n",
              "      <td>4.642771</td>\n",
              "      <td>5.161689</td>\n",
              "      <td>66.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4397</th>\n",
              "      <td>described</td>\n",
              "      <td>18.390805</td>\n",
              "      <td>3.773326</td>\n",
              "      <td>19.523008</td>\n",
              "      <td>6.354170</td>\n",
              "      <td>91.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4398</th>\n",
              "      <td>louisa</td>\n",
              "      <td>31.034483</td>\n",
              "      <td>4.800852</td>\n",
              "      <td>5.474415</td>\n",
              "      <td>11.835458</td>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4399</th>\n",
              "      <td>pursued</td>\n",
              "      <td>17.241379</td>\n",
              "      <td>4.412523</td>\n",
              "      <td>7.529468</td>\n",
              "      <td>5.185745</td>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4400 rows √ó 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad470571-8206-4fc1-b450-d8fbc8bc818d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ad470571-8206-4fc1-b450-d8fbc8bc818d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ad470571-8206-4fc1-b450-d8fbc8bc818d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           word       nFix       FFD        GPT        TRT     fixProp\n",
              "0           the   8.089145  2.037560   3.279184   2.620190   46.250203\n",
              "1            of   5.558040  1.492221   2.538022   1.791782   33.937491\n",
              "2           and   7.638862  1.941896   3.306894   2.450817   44.722639\n",
              "3             a   5.309940  1.491783   2.447444   1.775713   32.829983\n",
              "4            in   5.313665  1.489553   2.466216   1.798441   32.379519\n",
              "...         ...        ...       ...        ...        ...         ...\n",
              "4395    swedish  24.137931  4.171965   6.316368   7.849067   83.333333\n",
              "4396     curves  14.942529  3.625554   4.642771   5.161689   66.666667\n",
              "4397  described  18.390805  3.773326  19.523008   6.354170   91.666667\n",
              "4398     louisa  31.034483  4.800852   5.474415  11.835458  100.000000\n",
              "4399    pursued  17.241379  4.412523   7.529468   5.185745  100.000000\n",
              "\n",
              "[4400 rows x 6 columns]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_uniq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIzHxDNwegZW",
        "outputId": "c735dfc6-77d2-4780-bdb1-8d40d8809cf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['word', 'nFix', 'FFD', 'GPT', 'TRT', 'fixProp'], dtype='object')"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "howdf_uniq.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n8_nOrKcjml"
      },
      "outputs": [],
      "source": [
        "gaze ={}\n",
        "for ind, item in df_uniq.iterrows():\n",
        "  gaze[item[0]] = np.array(item[1:], dtype = np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGrRWCnVf9Is",
        "outputId": "e7312de5-21b8-4678-9e32-4a9de8a84d10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4400"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(gaze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBXM549ucUdj"
      },
      "outputs": [],
      "source": [
        "# Twitter_glove\n",
        "\n",
        "matched_tokens = {}\n",
        "missed_tokens = {}\n",
        "\n",
        "for profile in X:\n",
        "    for tweet in profile:\n",
        "        tokens = set(\n",
        "            twitter_tokenizer.tokenize(tweet.lower()))\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                matched_tokens[token] = gaze[token]\n",
        "            except KeyError:\n",
        "                missed_tokens[token] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkBdeEcHg7hN",
        "outputId": "c7fc2941-395a-4536-8a5e-b90aa2cfaae2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3911, 73364)"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens), len(missed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hv54BASg91q",
        "outputId": "09302426-c299-4aa8-a21e-64a22d7f6403"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.050611452604335166"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens) / (len(matched_tokens) + len(missed_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYxhTS2pkuaI"
      },
      "outputs": [],
      "source": [
        "get_gaze_embedding = lambda profiles, tokenizer: \\\n",
        "    get_embedding(profiles=profiles, tokenizer=tokenizer, \n",
        "        embedding_dict= gaze, embedding_dim= 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW32Q7euUQUs"
      },
      "outputs": [],
      "source": [
        "# MWE\n",
        "\n",
        "matched_tokens = {}\n",
        "missed_tokens = {}\n",
        "\n",
        "for profile in X:\n",
        "    for tweet in profile:\n",
        "        tokens = set(\n",
        "            mwe_tokenizer.tokenize(word_tokenize(tweet.lower())))\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                matched_tokens[token] = glove_dict[token]\n",
        "            except KeyError:\n",
        "                missed_tokens[token] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggNuuft3UQUs",
        "outputId": "c9bdb82e-65af-4554-8b13-7ed2bdb518b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50288, 34676)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens), len(missed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U0qG29bUQUs",
        "outputId": "d27b1946-3bb8-4331-97d1-f6d1ab0a7c0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5918742055458782"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens) / (len(matched_tokens) + len(missed_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSQbZWGqUQUs"
      },
      "source": [
        "##### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruL5DLhgUQUs",
        "outputId": "a5f0c2d6-09ab-476d-8225-9cdd4e8f3db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.75 \t AVG RF: 0.6309523809523809 \t NORM RF: 0.6190476190476191\n",
            "RF: 0.7023809523809523 \t AVG RF: 0.7738095238095238 \t NORM RF: 0.7380952380952381\n",
            "RF: 0.6666666666666666 \t AVG RF: 0.7142857142857143 \t NORM RF: 0.7142857142857143\n",
            "RF: 0.6785714285714286 \t AVG RF: 0.6428571428571429 \t NORM RF: 0.6190476190476191\n",
            "RF: 0.6785714285714286 \t AVG RF: 0.6904761904761905 \t NORM RF: 0.6190476190476191\n",
            "\n",
            "AVG SCORE: [0.6952381  0.69047619 0.66190476]\n"
          ]
        }
      ],
      "source": [
        "# Twitter Tokenizer | gaze\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_gaze_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    forest.fit(train_embedding, train_labels_cv)\n",
        "    avg_forest.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_forest.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_gaze_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    rf_score = forest.score(test_embedding, test_labels_cv)\n",
        "    avg_rf_score = avg_forest.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_rf_score = norm_forest.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((rf_score, avg_rf_score, norm_rf_score))\n",
        "\n",
        "    print('RF:', rf_score, '\\t', 'AVG RF:', avg_rf_score, '\\t',\n",
        "          'NORM RF:', norm_rf_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ6izLY6sT7j",
        "outputId": "c358ccc4-5976-4239-a357-369bfa2178f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9404761904761905 \t AVG RF: 0.9166666666666666 \t NORM RF: 0.9047619047619048\n",
            "RF: 0.8690476190476191 \t AVG RF: 0.9047619047619048 \t NORM RF: 0.9285714285714286\n",
            "RF: 0.8452380952380952 \t AVG RF: 0.8809523809523809 \t NORM RF: 0.8809523809523809\n",
            "RF: 0.8571428571428571 \t AVG RF: 0.8690476190476191 \t NORM RF: 0.8690476190476191\n",
            "RF: 0.8690476190476191 \t AVG RF: 0.9166666666666666 \t NORM RF: 0.9166666666666666\n",
            "\n",
            "AVG SCORE: [0.87619048 0.89761905 0.9       ]\n"
          ]
        }
      ],
      "source": [
        "# Twitter Tokenizer | concatenation: glove + gaze\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_gaze_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    train_embedding_gl, train_avg_embedding_gl, train_norm_embedding_gl = \\\n",
        "        get_glove_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "    \n",
        "    train_cv = np.hstack((train_embedding, train_embedding_gl))\n",
        "    train_avg_cv = np.hstack((train_avg_embedding, train_avg_embedding_gl))\n",
        "    train_norm_cv = np.hstack((train_norm_embedding, train_norm_embedding_gl))\n",
        "    \n",
        "\n",
        "    forest.fit(train_cv, train_labels_cv)\n",
        "    avg_forest.fit(train_avg_cv, train_labels_cv)\n",
        "    norm_forest.fit(train_norm_cv, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_gaze_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    test_embedding_gl, test_avg_embedding_gl, test_norm_embedding_gl = \\\n",
        "        get_glove_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    test_cv = np.hstack((test_embedding, test_embedding_gl))\n",
        "    test_avg_cv = np.hstack((test_avg_embedding, test_avg_embedding_gl))\n",
        "    test_norm_cv = np.hstack((test_norm_embedding, test_norm_embedding_gl))\n",
        "\n",
        "    rf_score = forest.score(test_cv, test_labels_cv)\n",
        "    avg_rf_score = avg_forest.score(test_avg_cv, test_labels_cv)\n",
        "    norm_rf_score = norm_forest.score(test_norm_cv, test_labels_cv)\n",
        "    scores.append((rf_score, avg_rf_score, norm_rf_score))\n",
        "\n",
        "    print('RF:', rf_score, '\\t', 'AVG RF:', avg_rf_score, '\\t',\n",
        "          'NORM RF:', norm_rf_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHCQ_iQMUQUs",
        "outputId": "6288248c-e073-489f-de8b-3420893607f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.85      0.92      0.88        37\n",
            "          NI       0.93      0.87      0.90        47\n",
            "\n",
            "    accuracy                           0.89        84\n",
            "   macro avg       0.89      0.90      0.89        84\n",
            "weighted avg       0.90      0.89      0.89        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "_, _, train_embedding = get_glove_embedding(X_train, twitter_tokenizer.tokenize)\n",
        "forest.fit(train_embedding, y_train)\n",
        "_, _, test_embedding = get_glove_embedding(X_test, twitter_tokenizer.tokenize)\n",
        "\n",
        "rf_predict = forest.predict(test_embedding)\n",
        "print(classification_report(y_test, rf_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXxEMEKcUQUs",
        "outputId": "579fc83f-3244-45f5-daae-110c6a512797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9285714285714286 \t AVG RF: 0.9404761904761905 \t NORM RF: 0.9285714285714286\n",
            "RF: 0.9523809523809523 \t AVG RF: 0.9523809523809523 \t NORM RF: 0.9642857142857143\n",
            "RF: 0.8690476190476191 \t AVG RF: 0.8809523809523809 \t NORM RF: 0.8928571428571429\n",
            "RF: 0.8809523809523809 \t AVG RF: 0.8452380952380952 \t NORM RF: 0.8690476190476191\n",
            "RF: 0.9285714285714286 \t AVG RF: 0.9523809523809523 \t NORM RF: 0.9166666666666666\n",
            "\n",
            "AVG SCORE: [0.91190476 0.91428571 0.91428571]\n"
          ]
        }
      ],
      "source": [
        "# MWE Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_glove_embedding(train_data_cv,\n",
        "            lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    forest.fit(train_embedding, train_labels_cv)\n",
        "    avg_forest.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_forest.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_glove_embedding(test_data_cv,\n",
        "            lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    rf_score = forest.score(test_embedding, test_labels_cv)\n",
        "    avg_rf_score = avg_forest.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_rf_score = norm_forest.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((rf_score, avg_rf_score, norm_rf_score))\n",
        "\n",
        "    print('RF:', rf_score, '\\t', 'AVG RF:', avg_rf_score, '\\t',\n",
        "          'NORM RF:', norm_rf_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DIhNYw_UQUs",
        "outputId": "1b141ed5-095e-4b22-f0c3-708bddbe44eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.94      0.89      0.92        37\n",
            "          NI       0.92      0.96      0.94        47\n",
            "\n",
            "    accuracy                           0.93        84\n",
            "   macro avg       0.93      0.92      0.93        84\n",
            "weighted avg       0.93      0.93      0.93        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "_, _, train_embedding = get_glove_embedding(\n",
        "    X_train, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "forest.fit(train_embedding, y_train)\n",
        "_, _, test_embedding = get_glove_embedding(\n",
        "        X_test, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "rf_predict = forest.predict(test_embedding)\n",
        "print(classification_report(y_test, rf_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1bCr31xUQUt"
      },
      "source": [
        "##### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4bvzWIoUQUt"
      },
      "outputs": [],
      "source": [
        "svm = SVC(kernel='rbf', C=1)\n",
        "avg_svm = SVC(kernel='rbf', C=1)\n",
        "norm_svm = SVC(kernel='rbf', C=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REmDTknWUQUt",
        "outputId": "29a07644-b938-4501-ef62-98e0c7452980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "svm: 0.7142857142857143 \t AVG svm: 0.44047619047619047 \t NORM svm: 0.44047619047619047\n",
            "svm: 0.6547619047619048 \t AVG svm: 0.5833333333333334 \t NORM svm: 0.5476190476190477\n",
            "svm: 0.6785714285714286 \t AVG svm: 0.4880952380952381 \t NORM svm: 0.47619047619047616\n",
            "svm: 0.6190476190476191 \t AVG svm: 0.5119047619047619 \t NORM svm: 0.47619047619047616\n",
            "svm: 0.6785714285714286 \t AVG svm: 0.44047619047619047 \t NORM svm: 0.44047619047619047\n",
            "\n",
            "AVG SCORE: [0.66904762 0.49285714 0.47619048]\n"
          ]
        }
      ],
      "source": [
        "# Twitter Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_gaze_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    svm.fit(train_embedding, train_labels_cv)\n",
        "    avg_svm.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_svm.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_gaze_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    svm_score = svm.score(test_embedding, test_labels_cv)\n",
        "    avg_svm_score = avg_svm.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_svm_score = norm_svm.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((svm_score, avg_svm_score, norm_svm_score))\n",
        "\n",
        "    print('svm:', svm_score, '\\t', 'AVG svm:', avg_svm_score, '\\t',\n",
        "          'NORM svm:', norm_svm_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe-suI0dt58I",
        "outputId": "aa4e5343-c369-443c-9e98-d41104a96c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.6428571428571429 \t AVG RF: 0.44047619047619047 \t NORM RF: 0.5714285714285714\n",
            "RF: 0.6428571428571429 \t AVG RF: 0.5833333333333334 \t NORM RF: 0.5833333333333334\n",
            "RF: 0.6666666666666666 \t AVG RF: 0.47619047619047616 \t NORM RF: 0.6547619047619048\n",
            "RF: 0.6071428571428571 \t AVG RF: 0.47619047619047616 \t NORM RF: 0.5952380952380952\n",
            "RF: 0.6904761904761905 \t AVG RF: 0.44047619047619047 \t NORM RF: 0.6785714285714286\n",
            "\n",
            "AVG SCORE: [0.65       0.48333333 0.61666667]\n"
          ]
        }
      ],
      "source": [
        "# Twitter Tokenizer | concatenation: glove + gaze\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_gaze_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    train_embedding_gl, train_avg_embedding_gl, train_norm_embedding_gl = \\\n",
        "        get_glove_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "    \n",
        "    train_cv = np.hstack((train_embedding, train_embedding_gl))\n",
        "    train_avg_cv = np.hstack((train_avg_embedding, train_avg_embedding_gl))\n",
        "    train_norm_cv = np.hstack((train_norm_embedding, train_norm_embedding_gl))\n",
        "    \n",
        "\n",
        "    svm.fit(train_cv, train_labels_cv)\n",
        "    avg_svm.fit(train_avg_cv, train_labels_cv)\n",
        "    norm_svm.fit(train_norm_cv, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_gaze_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    test_embedding_gl, test_avg_embedding_gl, test_norm_embedding_gl = \\\n",
        "        get_glove_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    test_cv = np.hstack((test_embedding, test_embedding_gl))\n",
        "    test_avg_cv = np.hstack((test_avg_embedding, test_avg_embedding_gl))\n",
        "    test_norm_cv = np.hstack((test_norm_embedding, test_norm_embedding_gl))\n",
        "\n",
        "    svm_score = svm.score(test_cv, test_labels_cv)\n",
        "    avg_svm_score = avg_svm.score(test_avg_cv, test_labels_cv)\n",
        "    norm_svm_score = norm_svm.score(test_norm_cv, test_labels_cv)\n",
        "    scores.append((svm_score, avg_svm_score, norm_svm_score))\n",
        "\n",
        "    print('RF:', svm_score, '\\t', 'AVG RF:', avg_svm_score, '\\t',\n",
        "          'NORM RF:', norm_svm_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijtexT8NUQUt",
        "outputId": "c49eab23-53e8-47bc-ad6d-1dfd2688711b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.79      0.81      0.80        37\n",
            "          NI       0.85      0.83      0.84        47\n",
            "\n",
            "    accuracy                           0.82        84\n",
            "   macro avg       0.82      0.82      0.82        84\n",
            "weighted avg       0.82      0.82      0.82        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "train_embedding,_,_ = get_glove_embedding(X_train, twitter_tokenizer.tokenize)\n",
        "svm.fit(train_embedding, y_train)\n",
        "test_embedding,_,_ = get_glove_embedding(X_test, twitter_tokenizer.tokenize)\n",
        "rf_predict = svm.predict(test_embedding)\n",
        "print(classification_report(y_test, rf_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um7ew99TUQUt",
        "outputId": "795a57a7-7f26-45c9-f209-540b4abf29c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "svm: 0.7857142857142857 \t AVG svm: 0.5833333333333334 \t NORM svm: 0.5833333333333334\n",
            "svm: 0.7380952380952381 \t AVG svm: 0.6071428571428571 \t NORM svm: 0.6190476190476191\n",
            "svm: 0.7380952380952381 \t AVG svm: 0.6309523809523809 \t NORM svm: 0.6785714285714286\n",
            "svm: 0.7261904761904762 \t AVG svm: 0.5952380952380952 \t NORM svm: 0.6666666666666666\n",
            "svm: 0.7619047619047619 \t AVG svm: 0.75 \t NORM svm: 0.8095238095238095\n",
            "\n",
            "AVG SCORE: [0.75       0.63333333 0.67142857]\n"
          ]
        }
      ],
      "source": [
        "# MWE Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_glove_embedding(\n",
        "            train_data_cv, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    svm.fit(train_embedding, train_labels_cv)\n",
        "    avg_svm.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_svm.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_glove_embedding(\n",
        "            test_data_cv, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    svm_score = svm.score(test_embedding, test_labels_cv)\n",
        "    avg_svm_score = avg_svm.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_svm_score = norm_svm.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((svm_score, avg_svm_score, norm_svm_score))\n",
        "\n",
        "    print('svm:', svm_score, '\\t', 'AVG svm:', avg_svm_score, '\\t',\n",
        "          'NORM svm:', norm_svm_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch5beVmHUQUt",
        "outputId": "551e0d13-5e90-4a5e-960f-b99d9bfc3399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.70      0.86      0.77        37\n",
            "          NI       0.87      0.70      0.78        47\n",
            "\n",
            "    accuracy                           0.77        84\n",
            "   macro avg       0.78      0.78      0.77        84\n",
            "weighted avg       0.79      0.77      0.77        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "train_embedding,_,_ = get_glove_embedding(\n",
        "    X_train, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "svm.fit(train_embedding, y_train)\n",
        "test_embedding,_,_ = get_glove_embedding(\n",
        "    X_test, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "rf_predict = svm.predict(test_embedding)\n",
        "print(classification_report(y_test, rf_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFszaKuKUQUt"
      },
      "source": [
        "##### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tef7EuSCUQUt"
      },
      "outputs": [],
      "source": [
        "logistic_regression = LogisticRegression(C=1)\n",
        "avg_logistic_regression = LogisticRegression(C=1)\n",
        "norm_logistic_regression = LogisticRegression(C=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF99BnMnUQUt",
        "outputId": "8b97932c-8304-4b5d-a38c-84df1879de54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.6190476190476191 \t AVG logistic_regression: 0.5476190476190477 \t NORM logistic_regression: 0.44047619047619047\n",
            "logistic_regression: 0.7261904761904762 \t AVG logistic_regression: 0.6666666666666666 \t NORM logistic_regression: 0.6547619047619048\n",
            "logistic_regression: 0.6666666666666666 \t AVG logistic_regression: 0.6428571428571429 \t NORM logistic_regression: 0.47619047619047616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.75 \t AVG logistic_regression: 0.6309523809523809 \t NORM logistic_regression: 0.47619047619047616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.6071428571428571 \t AVG logistic_regression: 0.6666666666666666 \t NORM logistic_regression: 0.44047619047619047\n",
            "\n",
            "AVG SCORE: [0.67380952 0.63095238 0.49761905]\n"
          ]
        }
      ],
      "source": [
        "# Twitter Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_gaze_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    logistic_regression.fit(train_embedding, train_labels_cv)\n",
        "    avg_logistic_regression.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_logistic_regression.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_gaze_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    logistic_regression_score = logistic_regression.score(test_embedding, test_labels_cv)\n",
        "    avg_logistic_regression_score = avg_logistic_regression.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_logistic_regression_score = norm_logistic_regression.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((logistic_regression_score, avg_logistic_regression_score, norm_logistic_regression_score))\n",
        "\n",
        "    print('logistic_regression:', logistic_regression_score, '\\t', \n",
        "          'AVG logistic_regression:', avg_logistic_regression_score, '\\t',\n",
        "          'NORM logistic_regression:', norm_logistic_regression_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K99FPJJXuYnX",
        "outputId": "d0bf3d43-3d2a-4559-e805-abcc76705070"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8928571428571429 \t AVG RF: 0.7142857142857143 \t NORM RF: 0.5476190476190477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9166666666666666 \t AVG RF: 0.8095238095238095 \t NORM RF: 0.6309523809523809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8571428571428571 \t AVG RF: 0.7380952380952381 \t NORM RF: 0.6547619047619048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8571428571428571 \t AVG RF: 0.8333333333333334 \t NORM RF: 0.6428571428571429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9166666666666666 \t AVG RF: 0.8809523809523809 \t NORM RF: 0.75\n",
            "\n",
            "AVG SCORE: [0.88809524 0.7952381  0.6452381 ]\n"
          ]
        }
      ],
      "source": [
        "# Twitter Tokenizer | concatenation: glove + gaze\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_gaze_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    train_embedding_gl, train_avg_embedding_gl, train_norm_embedding_gl = \\\n",
        "        get_glove_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "    \n",
        "    train_cv = np.hstack((train_embedding, train_embedding_gl))\n",
        "    train_avg_cv = np.hstack((train_avg_embedding, train_avg_embedding_gl))\n",
        "    train_norm_cv = np.hstack((train_norm_embedding, train_norm_embedding_gl))\n",
        "    \n",
        "\n",
        "    logistic_regression.fit(train_cv, train_labels_cv)\n",
        "    avg_logistic_regression.fit(train_avg_cv, train_labels_cv)\n",
        "    norm_logistic_regression.fit(train_norm_cv, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_gaze_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    test_embedding_gl, test_avg_embedding_gl, test_norm_embedding_gl = \\\n",
        "        get_glove_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    test_cv = np.hstack((test_embedding, test_embedding_gl))\n",
        "    test_avg_cv = np.hstack((test_avg_embedding, test_avg_embedding_gl))\n",
        "    test_norm_cv = np.hstack((test_norm_embedding, test_norm_embedding_gl))\n",
        "\n",
        "    logistic_regression_score = logistic_regression.score(test_cv, test_labels_cv)\n",
        "    avg_logistic_regression_score = avg_logistic_regression.score(test_avg_cv, test_labels_cv)\n",
        "    norm_logistic_regression_score = norm_logistic_regression.score(test_norm_cv, test_labels_cv)\n",
        "    scores.append((logistic_regression_score, avg_logistic_regression_score, norm_logistic_regression_score))\n",
        "\n",
        "    print('RF:', logistic_regression_score, '\\t', 'AVG RF:', avg_logistic_regression_score, '\\t',\n",
        "          'NORM RF:', norm_logistic_regression_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1DvmuCzUQUt",
        "outputId": "0f42d7d9-0c1a-483f-8aee-46eeeb760e84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.79      0.89      0.84        37\n",
            "          NI       0.90      0.81      0.85        47\n",
            "\n",
            "    accuracy                           0.85        84\n",
            "   macro avg       0.85      0.85      0.84        84\n",
            "weighted avg       0.85      0.85      0.85        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "train_embedding,_,_ = get_glove_embedding(X_train, twitter_tokenizer.tokenize)\n",
        "logistic_regression.fit(train_embedding, y_train)\n",
        "test_embedding,_,_ = get_glove_embedding(X_test, twitter_tokenizer.tokenize)\n",
        "lr_predict = logistic_regression.predict(test_embedding)\n",
        "print(classification_report(y_test, lr_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQxUktTUUQUu",
        "outputId": "c42f933e-794b-4f95-ab7f-78699c066fbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8690476190476191 \t AVG logistic_regression: 0.8928571428571429 \t NORM logistic_regression: 0.5952380952380952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.9285714285714286 \t AVG logistic_regression: 0.9285714285714286 \t NORM logistic_regression: 0.7023809523809523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.9166666666666666 \t AVG logistic_regression: 0.8095238095238095 \t NORM logistic_regression: 0.7023809523809523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8214285714285714 \t AVG logistic_regression: 0.8452380952380952 \t NORM logistic_regression: 0.7380952380952381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8690476190476191 \t AVG logistic_regression: 0.9047619047619048 \t NORM logistic_regression: 0.7619047619047619\n",
            "\n",
            "AVG SCORE: [0.88095238 0.87619048 0.7       ]\n"
          ]
        }
      ],
      "source": [
        "# MWE Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_glove_embedding(\n",
        "            train_data_cv, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    logistic_regression.fit(train_embedding, train_labels_cv)\n",
        "    avg_logistic_regression.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_logistic_regression.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_glove_embedding(\n",
        "            test_data_cv, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    logistic_regression_score = logistic_regression.score(test_embedding, test_labels_cv)\n",
        "    avg_logistic_regression_score = avg_logistic_regression.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_logistic_regression_score = norm_logistic_regression.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((logistic_regression_score, avg_logistic_regression_score, norm_logistic_regression_score))\n",
        "\n",
        "    print('logistic_regression:', logistic_regression_score, '\\t', \n",
        "          'AVG logistic_regression:', avg_logistic_regression_score, '\\t',\n",
        "          'NORM logistic_regression:', norm_logistic_regression_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33Cee9r3UQUu",
        "outputId": "025ea769-7063-4d6e-cdb2-914e3f614860"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.83      0.92      0.87        37\n",
            "          NI       0.93      0.85      0.89        47\n",
            "\n",
            "    accuracy                           0.88        84\n",
            "   macro avg       0.88      0.88      0.88        84\n",
            "weighted avg       0.89      0.88      0.88        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "train_embedding,_,_ = get_glove_embedding(\n",
        "    X_train, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "logistic_regression.fit(train_embedding, y_train)\n",
        "test_embedding,_,_ = get_glove_embedding(\n",
        "    X_test, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "lr_predict = logistic_regression.predict(test_embedding)\n",
        "print(classification_report(y_test, lr_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqLV3oASUQUu"
      },
      "source": [
        "#### FastText Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tID2lYqbUQUu"
      },
      "outputs": [],
      "source": [
        "fast_en_path = '../data/fasten/fasttext_english_twitter_100d.vec'\n",
        "fast_en_dict, fast_en_counts, fast_en_dim = load_embedding(fast_en_path)\n",
        "get_fast_embedding = lambda profiles, tokenizer: \\\n",
        "    get_embedding(profiles=profiles, tokenizer=tokenizer, \n",
        "        embedding_dict=fast_en_dict, embedding_dim=fast_en_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE6lSIS_UQUu"
      },
      "outputs": [],
      "source": [
        "matched_tokens = {}\n",
        "missed_tokens = {}\n",
        "\n",
        "for profile in X:\n",
        "    for tweet in profile:\n",
        "        tokens = set(\n",
        "            twitter_tokenizer.tokenize(tweet.lower()))\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                matched_tokens[token] = fast_en_dict[token]\n",
        "            except KeyError:\n",
        "                missed_tokens[token] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LPqbdJtUQUu",
        "outputId": "88a1985b-b439-424e-f34a-8f40439303ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(51248, 26027)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens), len(missed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCo7Gp8rUQUu",
        "outputId": "a9ecf87b-2e0c-4a0c-eeeb-5a8006ca269f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.66318990617923"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens) / (len(matched_tokens) + len(missed_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga413dczUQUu"
      },
      "outputs": [],
      "source": [
        "# MWE\n",
        "\n",
        "matched_tokens = {}\n",
        "missed_tokens = {}\n",
        "\n",
        "for profile in X:\n",
        "    for tweet in profile:\n",
        "        tokens = set(\n",
        "            mwe_tokenizer.tokenize(word_tokenize(tweet.lower())))\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                matched_tokens[token] = fast_en_dict[token]\n",
        "            except KeyError:\n",
        "                missed_tokens[token] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHpMaLkeUQUu",
        "outputId": "74011a56-2550-44c0-dea7-f7d9dc59d91b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(48973, 35991)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens), len(missed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nct61oZUQUu",
        "outputId": "55acb984-aa12-4f29-d396-8f3f6b04617c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5763970622852032"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(matched_tokens) / (len(matched_tokens) + len(missed_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzwMxIV5UQUu"
      },
      "source": [
        "##### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkvZuf-QUQUu",
        "outputId": "408ecf6b-ce0f-48e4-a326-e02cc774b079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9285714285714286 \t AVG RF: 0.9523809523809523 \t NORM RF: 0.9523809523809523\n",
            "RF: 0.8571428571428571 \t AVG RF: 0.9404761904761905 \t NORM RF: 0.9523809523809523\n",
            "RF: 0.8571428571428571 \t AVG RF: 0.8809523809523809 \t NORM RF: 0.8809523809523809\n",
            "RF: 0.8571428571428571 \t AVG RF: 0.9047619047619048 \t NORM RF: 0.9166666666666666\n",
            "RF: 0.9166666666666666 \t AVG RF: 0.9166666666666666 \t NORM RF: 0.9285714285714286\n",
            "\n",
            "AVG SCORE: [0.88333333 0.91904762 0.92619048]\n"
          ]
        }
      ],
      "source": [
        "# Twitter Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_fast_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    forest.fit(train_embedding, train_labels_cv)\n",
        "    avg_forest.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_forest.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_fast_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    rf_score = forest.score(test_embedding, test_labels_cv)\n",
        "    avg_rf_score = avg_forest.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_rf_score = norm_forest.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((rf_score, avg_rf_score, norm_rf_score))\n",
        "\n",
        "    print('RF:', rf_score, '\\t', 'AVG RF:', avg_rf_score, '\\t',\n",
        "          'NORM RF:', norm_rf_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP83dWHaUQUv",
        "outputId": "e9f1d35e-7878-4ce8-dc90-39af2730a780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.92      0.95      0.93        37\n",
            "          NI       0.96      0.94      0.95        47\n",
            "\n",
            "    accuracy                           0.94        84\n",
            "   macro avg       0.94      0.94      0.94        84\n",
            "weighted avg       0.94      0.94      0.94        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "_, _, train_embedding = get_fast_embedding(X_train, twitter_tokenizer.tokenize)\n",
        "forest.fit(train_embedding, y_train)\n",
        "_, _, test_embedding = get_fast_embedding(X_test, twitter_tokenizer.tokenize)\n",
        "rf_predict = forest.predict(test_embedding)\n",
        "print(classification_report(y_test, rf_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj_d9UFSUQUv",
        "outputId": "c24ea888-4b9b-441d-dba6-f0eab26a6a6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9285714285714286 \t AVG RF: 0.9285714285714286 \t NORM RF: 0.9523809523809523\n",
            "RF: 0.8333333333333334 \t AVG RF: 0.9285714285714286 \t NORM RF: 0.9166666666666666\n",
            "RF: 0.8571428571428571 \t AVG RF: 0.9047619047619048 \t NORM RF: 0.9047619047619048\n",
            "RF: 0.8928571428571429 \t AVG RF: 0.8809523809523809 \t NORM RF: 0.9047619047619048\n",
            "RF: 0.9047619047619048 \t AVG RF: 0.9047619047619048 \t NORM RF: 0.8928571428571429\n",
            "\n",
            "AVG SCORE: [0.88333333 0.90952381 0.91428571]\n"
          ]
        }
      ],
      "source": [
        "# MWE Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_fast_embedding(train_data_cv,\n",
        "            lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    forest.fit(train_embedding, train_labels_cv)\n",
        "    avg_forest.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_forest.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_fast_embedding(test_data_cv,\n",
        "            lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    # prediction = forest.predict(test_embedding)\n",
        "    # index = np.where(prediction != test_labels_cv)[0]\n",
        "    # wrong_index.append(index)\n",
        "    rf_score = forest.score(test_embedding, test_labels_cv)\n",
        "    avg_rf_score = avg_forest.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_rf_score = norm_forest.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((rf_score, avg_rf_score, norm_rf_score))\n",
        "\n",
        "    print('RF:', rf_score, '\\t', 'AVG RF:', avg_rf_score, '\\t',\n",
        "          'NORM RF:', norm_rf_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89HTg2TBUQUv",
        "outputId": "0369fd07-6586-42c4-b810-1e99f17043aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.86      0.97      0.91        37\n",
            "          NI       0.98      0.87      0.92        47\n",
            "\n",
            "    accuracy                           0.92        84\n",
            "   macro avg       0.92      0.92      0.92        84\n",
            "weighted avg       0.92      0.92      0.92        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "_, _, train_embedding = get_fast_embedding(\n",
        "    X_train, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "forest.fit(train_embedding, y_train)\n",
        "_, _, test_embedding = get_fast_embedding(\n",
        "    X_test, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "rf_predict = forest.predict(test_embedding)\n",
        "print(classification_report(y_test, rf_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofPRFB4UUQUv"
      },
      "source": [
        "##### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyprmTwPUQUv",
        "outputId": "f69c58aa-5806-4585-d3f3-eb48d80aea02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "svm: 0.8809523809523809 \t AVG svm: 0.8095238095238095 \t NORM svm: 0.7976190476190477\n",
            "svm: 0.8452380952380952 \t AVG svm: 0.9047619047619048 \t NORM svm: 0.9047619047619048\n",
            "svm: 0.7380952380952381 \t AVG svm: 0.8690476190476191 \t NORM svm: 0.8690476190476191\n",
            "svm: 0.7976190476190477 \t AVG svm: 0.7976190476190477 \t NORM svm: 0.7976190476190477\n",
            "svm: 0.8333333333333334 \t AVG svm: 0.9166666666666666 \t NORM svm: 0.9166666666666666\n",
            "\n",
            "AVG SCORE: [0.81904762 0.85952381 0.85714286]\n"
          ]
        }
      ],
      "source": [
        "# Twitter Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_fast_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    svm.fit(train_embedding, train_labels_cv)\n",
        "    avg_svm.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_svm.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_fast_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    svm_score = svm.score(test_embedding, test_labels_cv)\n",
        "    avg_svm_score = avg_svm.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_svm_score = norm_svm.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((svm_score, avg_svm_score, norm_svm_score))\n",
        "\n",
        "    print('svm:', svm_score, '\\t', 'AVG svm:', avg_svm_score, '\\t',\n",
        "          'NORM svm:', norm_svm_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5Kx1cH0UQUv",
        "outputId": "7da30cfb-217b-4b34-e627-b1ca1ed40bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.68      0.92      0.78        37\n",
            "          NI       0.91      0.66      0.77        47\n",
            "\n",
            "    accuracy                           0.77        84\n",
            "   macro avg       0.80      0.79      0.77        84\n",
            "weighted avg       0.81      0.77      0.77        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "_,train_embedding,_ = get_fast_embedding(X_train, twitter_tokenizer.tokenize)\n",
        "svm.fit(train_embedding, y_train)\n",
        "_,test_embedding,_ = get_fast_embedding(X_test, twitter_tokenizer.tokenize)\n",
        "svm_predict = svm.predict(test_embedding)\n",
        "print(classification_report(y_test, svm_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuCwVEubUQUv",
        "outputId": "fba88355-745b-4f7b-c9c9-90bba863fd75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "svm: 0.8214285714285714 \t AVG svm: 0.6785714285714286 \t NORM svm: 0.6547619047619048\n",
            "svm: 0.7738095238095238 \t AVG svm: 0.7261904761904762 \t NORM svm: 0.7142857142857143\n",
            "svm: 0.7261904761904762 \t AVG svm: 0.7380952380952381 \t NORM svm: 0.7380952380952381\n",
            "svm: 0.7261904761904762 \t AVG svm: 0.6785714285714286 \t NORM svm: 0.6666666666666666\n",
            "svm: 0.75 \t AVG svm: 0.7857142857142857 \t NORM svm: 0.7857142857142857\n",
            "\n",
            "AVG SCORE: [0.75952381 0.72142857 0.71190476]\n"
          ]
        }
      ],
      "source": [
        "# MWE Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_fast_embedding(\n",
        "            train_data_cv, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    svm.fit(train_embedding, train_labels_cv)\n",
        "    avg_svm.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_svm.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_fast_embedding(\n",
        "            test_data_cv, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    svm_score = svm.score(test_embedding, test_labels_cv)\n",
        "    avg_svm_score = avg_svm.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_svm_score = norm_svm.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((svm_score, avg_svm_score, norm_svm_score))\n",
        "\n",
        "    print('svm:', svm_score, '\\t', 'AVG svm:', avg_svm_score, '\\t',\n",
        "          'NORM svm:', norm_svm_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imm2P2C2UQUv"
      },
      "source": [
        "##### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3for5nXCUQUv",
        "outputId": "78bb4a8c-7636-45cc-eb3d-3ad4ae3b47eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8809523809523809 \t AVG logistic_regression: 0.8690476190476191 \t NORM logistic_regression: 0.7380952380952381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8809523809523809 \t AVG logistic_regression: 0.8809523809523809 \t NORM logistic_regression: 0.8452380952380952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8809523809523809 \t AVG logistic_regression: 0.8333333333333334 \t NORM logistic_regression: 0.8214285714285714\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8809523809523809 \t AVG logistic_regression: 0.8214285714285714 \t NORM logistic_regression: 0.7976190476190477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8690476190476191 \t AVG logistic_regression: 0.8809523809523809 \t NORM logistic_regression: 0.8571428571428571\n",
            "\n",
            "AVG SCORE: [0.87857143 0.85714286 0.81190476]\n"
          ]
        }
      ],
      "source": [
        "# Twitter Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_fast_embedding(train_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    logistic_regression.fit(train_embedding, train_labels_cv)\n",
        "    avg_logistic_regression.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_logistic_regression.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_fast_embedding(test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    logistic_regression_score = logistic_regression.score(test_embedding, test_labels_cv)\n",
        "    avg_logistic_regression_score = avg_logistic_regression.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_logistic_regression_score = norm_logistic_regression.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((logistic_regression_score, avg_logistic_regression_score, norm_logistic_regression_score))\n",
        "\n",
        "    print('logistic_regression:', logistic_regression_score, '\\t', 'AVG logistic_regression:', avg_logistic_regression_score, '\\t',\n",
        "          'NORM logistic_regression:', norm_logistic_regression_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k7Z7RkkUQUw",
        "outputId": "10205618-bc18-4559-fed7-67d9d8e189b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.83      0.92      0.87        37\n",
            "          NI       0.93      0.85      0.89        47\n",
            "\n",
            "    accuracy                           0.88        84\n",
            "   macro avg       0.88      0.88      0.88        84\n",
            "weighted avg       0.89      0.88      0.88        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "train_embedding,_,_ = get_fast_embedding(X_train, twitter_tokenizer.tokenize)\n",
        "logistic_regression.fit(train_embedding, y_train)\n",
        "test_embedding,_,_ = get_fast_embedding(X_test, twitter_tokenizer.tokenize)\n",
        "lr_predict = logistic_regression.predict(test_embedding)\n",
        "print(classification_report(y_test, lr_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1lCb_2QUQUw",
        "outputId": "ee074fc8-f689-442f-c9a9-4c18ff1a9927"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8809523809523809 \t AVG logistic_regression: 0.8095238095238095 \t NORM logistic_regression: 0.6904761904761905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8571428571428571 \t AVG logistic_regression: 0.8214285714285714 \t NORM logistic_regression: 0.7261904761904762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8571428571428571 \t AVG logistic_regression: 0.7857142857142857 \t NORM logistic_regression: 0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8095238095238095 \t AVG logistic_regression: 0.7976190476190477 \t NORM logistic_regression: 0.7261904761904762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression: 0.8809523809523809 \t AVG logistic_regression: 0.8809523809523809 \t NORM logistic_regression: 0.8095238095238095\n",
            "\n",
            "AVG SCORE: [0.85714286 0.81904762 0.74047619]\n"
          ]
        }
      ],
      "source": [
        "# MWE Tokenizer\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "wrong_index = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv, test_labels_cv = y[train], y[test]\n",
        "\n",
        "    train_embedding, train_avg_embedding, train_norm_embedding = \\\n",
        "        get_fast_embedding(\n",
        "            train_data_cv, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    logistic_regression.fit(train_embedding, train_labels_cv)\n",
        "    avg_logistic_regression.fit(train_avg_embedding, train_labels_cv)\n",
        "    norm_logistic_regression.fit(train_norm_embedding, train_labels_cv)\n",
        "\n",
        "    test_embedding, test_avg_embedding, test_norm_embedding = \\\n",
        "        get_fast_embedding(\n",
        "            test_data_cv, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "\n",
        "    logistic_regression_score = logistic_regression.score(test_embedding, test_labels_cv)\n",
        "    avg_logistic_regression_score = avg_logistic_regression.score(test_avg_embedding, test_labels_cv)\n",
        "    norm_logistic_regression_score = norm_logistic_regression.score(test_norm_embedding, test_labels_cv)\n",
        "    scores.append((logistic_regression_score, avg_logistic_regression_score, norm_logistic_regression_score))\n",
        "\n",
        "    print('logistic_regression:', logistic_regression_score, '\\t', 'AVG logistic_regression:', avg_logistic_regression_score, '\\t',\n",
        "          'NORM logistic_regression:', norm_logistic_regression_score)\n",
        "\n",
        "print('\\nAVG SCORE:', np.mean(scores, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LQG70bhUQUw",
        "outputId": "8c11526e-2535-4479-d642-06b9d2c5c6b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.81      0.92      0.86        37\n",
            "          NI       0.93      0.83      0.88        47\n",
            "\n",
            "    accuracy                           0.87        84\n",
            "   macro avg       0.87      0.87      0.87        84\n",
            "weighted avg       0.88      0.87      0.87        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "train_embedding,_,_ = get_fast_embedding(\n",
        "    X_train, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "logistic_regression.fit(train_embedding, y_train)\n",
        "test_embedding,_,_ = get_fast_embedding(\n",
        "    X_test, lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))\n",
        "lr_predict = logistic_regression.predict(test_embedding)\n",
        "print(classification_report(y_test, lr_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLOF0LKVUQUw"
      },
      "source": [
        "### FastText + Non-embedding Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-diZA8RUQUw",
        "outputId": "8e188fbc-312a-432b-f82b-f72db1b42950"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9642857142857143 \t SVM: 0.9047619047619048 \t LR: 0.9166666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9404761904761905 \t SVM: 0.9047619047619048 \t LR: 0.9404761904761905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.8928571428571429 \t SVM: 0.8809523809523809 \t LR: 0.9047619047619048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9047619047619048 \t SVM: 0.8571428571428571 \t LR: 0.8928571428571429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/langproc2/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: 0.9404761904761905 \t SVM: 0.8809523809523809 \t LR: 0.9285714285714286\n",
            "\n",
            "AVG RF SCORE: 0.9285714285714285 \n",
            "AVG SVM SCORE: 0.8857142857142858 \n",
            "AVG LR SCORE: 0.9166666666666666\n"
          ]
        }
      ],
      "source": [
        "# No Optimization; Time Killer!\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = []\n",
        "\n",
        "for train, test in cv.split(X):\n",
        "    train_data_cv, test_data_cv = X[train], X[test]\n",
        "    train_labels_cv_original, test_labels_cv_original = y[train], y[test]\n",
        "\n",
        "    bert_tokens_dict = {}\n",
        "    for user in train_data_cv:\n",
        "        for tweet in user:\n",
        "            tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "            for token in tokens:\n",
        "                bert_tokens_dict[token] = 0\n",
        "\n",
        "    train_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in train_labels_cv_original])\n",
        "    test_labels_cv = np.array(\n",
        "        [1 if label == 'I' else 0 for label in test_labels_cv_original])\n",
        "\n",
        "    trains_vector_cv = np.zeros((\n",
        "        len(train_data_cv), len(bert_tokens_dict)))\n",
        "    for index, user in enumerate(train_data_cv):\n",
        "        user_tokens = copy.deepcopy(bert_tokens_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "            for token in tokens:\n",
        "                user_tokens[token] += 1\n",
        "        trains_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    train_emoji_usage = np.array(\n",
        "        [get_emoji_usage(user)[0] for user in train_data_cv])\n",
        "    \n",
        "    train_average_length = np.mean(\n",
        "        np.array([get_tweets_length(user) for user in train_data_cv]),\n",
        "        axis=1).reshape(-1, 1)\n",
        "    \n",
        "    _, _, train_embedding = get_fast_embedding(\n",
        "        train_data_cv, twitter_tokenizer.tokenize)\n",
        "    \n",
        "    train_cv = np.hstack((trains_vector_cv,\n",
        "        train_emoji_usage, train_average_length, train_embedding))\n",
        "\n",
        "    forest.fit(train_cv, train_labels_cv)\n",
        "    svm.fit(train_cv, train_labels_cv)\n",
        "    logistic_regression.fit(train_cv, train_labels_cv)\n",
        "\n",
        "    tests_vector_cv = np.zeros((\n",
        "        len(test_data_cv), len(bert_tokens_dict)))\n",
        "    for index, user in enumerate(test_data_cv):\n",
        "        user_tokens = copy.deepcopy(bert_tokens_dict)\n",
        "        for tweet in user:\n",
        "            tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "            for token in tokens:\n",
        "                try:\n",
        "                    user_tokens[token] += 1\n",
        "                except KeyError:\n",
        "                    pass\n",
        "        tests_vector_cv[index] = np.array(\n",
        "            list(user_tokens.values()))\n",
        "\n",
        "    test_emoji_usage = np.array(\n",
        "        [get_emoji_usage(user)[0] for user in test_data_cv])\n",
        "\n",
        "    test_average_length = np.mean(\n",
        "        np.array([get_tweets_length(user) for user in test_data_cv]),\n",
        "        axis=1).reshape(-1, 1)\n",
        "\n",
        "    _, _, test_embedding = get_fast_embedding(\n",
        "        test_data_cv, twitter_tokenizer.tokenize)\n",
        "\n",
        "    test_cv = np.hstack((tests_vector_cv,\n",
        "        test_emoji_usage, test_average_length, test_embedding))\n",
        "    \n",
        "    forest_score = forest.score(test_cv, test_labels_cv)\n",
        "    svm_score = svm.score(test_cv, test_labels_cv)\n",
        "    logistic_regression_score = logistic_regression.score(\n",
        "        test_cv, test_labels_cv)\n",
        "    \n",
        "    print('RF:', forest_score, '\\t', 'SVM:', svm_score, '\\t', \n",
        "          'LR:', logistic_regression_score)\n",
        "\n",
        "    # index = np.where(prediction != test_labels_cv)[0]\n",
        "    # wrong_index.append(index)\n",
        "    # print(classification_report(test_labels_cv, prediction))\n",
        "\n",
        "    scores.append((forest_score, svm_score, logistic_regression_score))\n",
        "\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "print('\\nAVG RF SCORE:', mean_scores[0],\n",
        "      '\\nAVG SVM SCORE:', mean_scores[1],\n",
        "      '\\nAVG LR SCORE:', mean_scores[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEg7oZCiUQUw",
        "outputId": "191ef958-df85-48d6-8716-a39b272d05ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.95      0.97      0.96        37\n",
            "          NI       0.98      0.96      0.97        47\n",
            "\n",
            "    accuracy                           0.96        84\n",
            "   macro avg       0.96      0.97      0.96        84\n",
            "weighted avg       0.96      0.96      0.96        84\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bert_tokens_dict = {}\n",
        "for user in X_train:\n",
        "    for tweet in user:\n",
        "        tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "        for token in tokens:\n",
        "            bert_tokens_dict[token] = 0\n",
        "\n",
        "train_token_vector = np.zeros((\n",
        "    len(X_train), len(bert_tokens_dict)))\n",
        "\n",
        "for index, user in enumerate(X_train):\n",
        "    user_tokens = copy.deepcopy(bert_tokens_dict)\n",
        "    for tweet in user:\n",
        "        tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "        for token in tokens:\n",
        "            user_tokens[token] += 1\n",
        "    train_token_vector[index] = np.array(\n",
        "        list(user_tokens.values()))\n",
        "\n",
        "_,_, train_embedding = get_fast_embedding(X_train, twitter_tokenizer.tokenize)\n",
        "\n",
        "train_emoji_usage = np.array(\n",
        "    [get_emoji_usage(user)[0] for user in X_train])\n",
        "    \n",
        "train_average_length = np.mean(\n",
        "    np.array([get_tweets_length(user) for user in X_train]),\n",
        "    axis=1).reshape(-1, 1)\n",
        "\n",
        "train_combine = np.hstack((train_token_vector, train_emoji_usage,\n",
        "    train_average_length, train_embedding))\n",
        "\n",
        "\n",
        "forest.fit(train_combine, y_train)\n",
        "\n",
        "\n",
        "test_token_vector = np.zeros((\n",
        "    len(X_test), len(bert_tokens_dict)))\n",
        "\n",
        "for index, user in enumerate(X_test):\n",
        "    user_tokens = copy.deepcopy(bert_tokens_dict)\n",
        "    for tweet in user:\n",
        "        tokens = set(bert_tokenizer.tokenize(tweet))\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                user_tokens[token] += 1\n",
        "            except KeyError:\n",
        "                pass\n",
        "    test_token_vector[index] = np.array(\n",
        "        list(user_tokens.values()))\n",
        "\n",
        "_,_, test_embedding = get_fast_embedding(X_test, twitter_tokenizer.tokenize)\n",
        "\n",
        "test_emoji_usage = np.array(\n",
        "    [get_emoji_usage(user)[0] for user in X_test])\n",
        "\n",
        "test_average_length = np.mean(\n",
        "    np.array([get_tweets_length(user) for user in X_test]),\n",
        "    axis=1).reshape(-1, 1)\n",
        "\n",
        "test_combine = np.hstack((test_token_vector, test_emoji_usage,\n",
        "    test_average_length, test_embedding))\n",
        "\n",
        "rf_predict = forest.predict(test_combine)\n",
        "\n",
        "print(classification_report(y_test, rf_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABerjSfsUQUw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "81c0a520ab9d5f38718f11fc8e39c528dfdc4be4c6d24f2eb9940d412ba4096a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
